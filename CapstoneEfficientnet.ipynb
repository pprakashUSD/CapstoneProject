{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "837994cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os.path\n",
    "import cv2\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display, Markdown\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from time import perf_counter\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow.keras.layers as tfl\n",
    "from keras.optimizers import SGD   \n",
    "from  sklearn.preprocessing import LabelEncoder \n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e67b81c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 754 Train Patient Samples: 632\n",
      "Test size: 4 Test Patient Samples: 4\n",
      "Other size: 396 Other Patient Samples: 336\n"
     ]
    }
   ],
   "source": [
    "df_original= pd.read_csv('/home/bear/prakash/aai541/finalproject/data/train.csv')\n",
    "df_test = pd.read_csv('/home/bear/prakash/aai541/finalproject/data/test.csv')\n",
    "df_other = pd.read_csv('/home/bear/prakash/aai541/finalproject/data/other.csv')\n",
    "df_original.shape\n",
    "print(\"Train size:\", len(df_original), \"Train Patient Samples:\", len(df_original.patient_id.unique()))\n",
    "print(\"Test size:\", len(df_test), \"Test Patient Samples:\", len(df_test.patient_id.unique()))\n",
    "print(\"Other size:\", len(df_other), \"Other Patient Samples:\", len(df_other.patient_id.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52760201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   image_id  center_id patient_id  image_num label\n",
      "0  006388_0         11     006388          0    CE\n",
      "1  008e5c_0         11     008e5c          0    CE\n",
      "2  00c058_0         11     00c058          0   LAA\n",
      "3  01adc5_0         11     01adc5          0   LAA\n",
      "4  026c97_0          4     026c97          0    CE\n",
      "                                            Filepath Label\n",
      "0  /home/bear/prakash/aai541/finalproject/data/tr...    CE\n",
      "1  /home/bear/prakash/aai541/finalproject/data/tr...    CE\n",
      "2  /home/bear/prakash/aai541/finalproject/data/tr...   LAA\n",
      "3  /home/bear/prakash/aai541/finalproject/data/tr...   LAA\n",
      "4  /home/bear/prakash/aai541/finalproject/data/tr...    CE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/PIL/Image.py:2896: DecompressionBombWarning: Image size (2934520020 pixels) exceeds limit of 1933120000 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# its bombing after certain size . Removing the size limit\n",
    "#Before\n",
    "print(df_original.head())\n",
    "# add images path to train and get width and height of images\n",
    "dfX=pd.DataFrame(columns=['Filepath', 'Label'])\n",
    "dfX[\"Filepath\"] = \"/home/bear/prakash/aai541/finalproject/data/train/\" + df_original[\"image_id\"] + \".tif\"\n",
    "dfX[\"Label\"] = df_original.label\n",
    "#After\n",
    "dfX = dfX[:100]\n",
    "print(dfX.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "93f70734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8586586550\n",
    "#\n",
    "# This loads images using Image Data Generator\n",
    "def create_gen(train_df, test_df):\n",
    "    # Load the Images with a generator and Data Augmentation\n",
    "    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input,\n",
    "        validation_split=0.1\n",
    "    )\n",
    "\n",
    "    test_generator = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        preprocessing_function=tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    )\n",
    "\n",
    "    train_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        subset='training',\n",
    "        rotation_range=30, # Uncomment to use data augmentation\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    val_images = train_generator.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "        subset='validation',\n",
    "        rotation_range=30, # Uncomment to use data augmentation\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.15,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode=\"nearest\"\n",
    "    )\n",
    "\n",
    "    test_images = test_generator.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        x_col='Filepath',\n",
    "        y_col='Label',\n",
    "        target_size=(224, 224),\n",
    "        color_mode='rgb',\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    return train_generator,test_generator,train_images,val_images,test_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "390814d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(learning_rate=0.001)\n",
    "def get_model(model):\n",
    "# Load the pretained model\n",
    "    kwargs =    {'input_shape':(224, 224, 3),\n",
    "                'include_top':False,\n",
    "                'weights':'imagenet',\n",
    "                'pooling':'avg'}\n",
    "    \n",
    "    pretrained_model = model(**kwargs)\n",
    "    pretrained_model.trainable = False\n",
    "    \n",
    "    inputs = pretrained_model.input\n",
    "\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(pretrained_model.output)\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    outputs = tf.keras.layers.Dense(36, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=sgd,#'adam',\n",
    "        loss= 'binary_crossentropy', #'categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abf9269e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL \n",
    "\n",
    "PIL.Image.MAX_IMAGE_PIXELS = 1933120000\n",
    "# Dictionary with the models\n",
    "models = {\n",
    "    \"EfficientNetB4\": {\"model\":tf.keras.applications.efficientnet.EfficientNetB4, \"perf\":0},    \n",
    "    \"VGG16\": {\"model\":tf.keras.applications.VGG16, \"perf\":0},\n",
    "    \"InceptionResNetV2\": {\"model\":tf.keras.applications.InceptionResNetV2, \"perf\":0},\n",
    "    \"ResNet101\": {\"model\":tf.keras.applications.ResNet101, \"perf\":0},\n",
    "    \"InceptionV3\": {\"model\":tf.keras.applications.InceptionV3, \"perf\":0},\n",
    "    \"MobileNet\": {\"model\":tf.keras.applications.MobileNet, \"perf\":0},\n",
    "}\n",
    "\n",
    "def buildTransferModels(TRAIN_IMAGES, VAL_IMAGES, name):\n",
    "    print(\"Getting model:\", name)\n",
    "    model = models.get(name)\n",
    "    print(\"processing:\", name)\n",
    "    # Get the model\n",
    "    m = get_model(model['model'])\n",
    "    models[name]['model'] = m\n",
    "    start = perf_counter()\n",
    "    print('fitting the model:', name)\n",
    "    history = m.fit(TRAIN_IMAGES,\n",
    "                    validation_data=VAL_IMAGES,\n",
    "                    epochs=10,\n",
    "                    verbose=2,\n",
    "                   batch_size = 32)\n",
    "\n",
    "    # Sav the duration, the train_accuracy and the val_accuracy\n",
    "    duration = perf_counter() - start\n",
    "    duration = round(duration,2)\n",
    "    models[name]['perf'] = duration\n",
    "    print(f\"{name:20} trained in {duration} sec\")\n",
    "    \n",
    "    val_acc = history.history['val_accuracy']\n",
    "    models[name]['val_acc'] = [round(v,4) for v in val_acc]\n",
    "    \n",
    "    train_acc = history.history['accuracy']\n",
    "    models[name]['train_accuracy'] = [round(v,4) for v in train_acc]\n",
    "    m.save(name+\"36.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a02f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Splitting data\n",
    "#X_train,X_val,Y_train,Y_val=train_test_split(images,labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "28e452b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2) (90, 2) (10, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#dfX = pd.DataFrame((images, labels))\n",
    "# Concatenate filepaths and labels\n",
    "#dfX = pd.concat([np.array(images), np.array(labels)], axis=1)\n",
    "# Separate in train and test data\n",
    "TRAIN_DF, TEST_DF = train_test_split(dfX, train_size=0.9, shuffle=True, random_state=1)\n",
    "print(dfX.shape, TRAIN_DF.shape, TEST_DF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b580db4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 81 validated image filenames belonging to 2 classes.\n",
      "Found 9 validated image filenames belonging to 2 classes.\n",
      "Found 10 validated image filenames belonging to 2 classes.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the generators\n",
    "train_generator,test_generator,TRAIN_IMAGES,VAL_IMAGES,TEST_IMAGES=create_gen(TRAIN_DF, TEST_DF)\n",
    "\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efe6169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting model: VGG16\n",
      "processing: VGG16\n",
      "fitting the model: VGG16\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-03 10:45:43.730363: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 408.00MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-03 10:45:43.730963: W tensorflow/core/kernels/gpu_utils.cc:50] Failed to allocate memory for convolution redzone checking; skipping this check. This is benign and only means that we won't check cudnn for out-of-bounds reads and writes. This message will only be printed once.\n",
      "2023-08-03 10:45:43.816482: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.02GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-03 10:45:43.904870: W tensorflow/tsl/framework/bfc_allocator.cc:290] Allocator (GPU_0_bfc) ran out of memory trying to allocate 939.37MiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2023-08-03 10:45:53.909216: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 392.00MiB (rounded to 411041792)requested by op Conv2D\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-08-03 10:45:53.914280: W tensorflow/tsl/framework/bfc_allocator.cc:492] *******************************************************************_********************************\n",
      "2023-08-03 10:45:53.914340: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at conv_ops.cc:1009 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[32,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'block1_conv2' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[32,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\n\nCall arguments received by layer 'block1_conv2' (type Conv2D):\n  • inputs=tf.Tensor(shape=(32, 224, 224, 64), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m saved \u001b[38;5;241m=\u001b[39m \u001b[43mbuildTransferModels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_IMAGES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVAL_IMAGES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVGG16\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#\"EfficientNetB4\") \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#saved = buildTransferModels(TRAIN_IMAGES, VAL_IMAGES)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m( \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA total of :\u001b[39m\u001b[38;5;124m\"\u001b[39m, saved,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m models from this build\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[44], line 23\u001b[0m, in \u001b[0;36mbuildTransferModels\u001b[0;34m(TRAIN_IMAGES, VAL_IMAGES, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfitting the model:\u001b[39m\u001b[38;5;124m'\u001b[39m, name)\n\u001b[0;32m---> 23\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_IMAGES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVAL_IMAGES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m               \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Sav the duration, the train_accuracy and the val_accuracy\u001b[39;00m\n\u001b[1;32m     30\u001b[0m duration \u001b[38;5;241m=\u001b[39m perf_counter() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'block1_conv2' (type Conv2D).\n\n{{function_node __wrapped__Conv2D_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[32,64,224,224] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Conv2D]\n\nCall arguments received by layer 'block1_conv2' (type Conv2D):\n  • inputs=tf.Tensor(shape=(32, 224, 224, 64), dtype=float32)"
     ]
    }
   ],
   "source": [
    "\n",
    "saved = buildTransferModels(TRAIN_IMAGES, VAL_IMAGES, \"VGG16\")#\"EfficientNetB4\") \n",
    "#saved = buildTransferModels(TRAIN_IMAGES, VAL_IMAGES)\n",
    "print( \"A total of :\", saved,\" models from this build\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af01be1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
